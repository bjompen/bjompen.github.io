# A long time coming...

..this post. I have done some cool stuff, as always, gotten to work a whole lot more with GitHub (Including getting some certifications from there ðŸ˜Š), spoken at a couple of conferences, have a couple more lined up, and joining new consultancies doing new super fun things.

Yet it still is hard.

Hard to write. Hard to enjoy it. Hard to feel that building is fun.

Why? Well... Because of "AI".

So I think it is time to write a bit about that.

## Lets start at the beginning

In the beginning, there was AI. You may think this is a new fancy thing, but oh boy you're wrong. AI is about as old as computers themselves. The infamous [Touring test](https://en.wikipedia.org/wiki/Turing_test) was defined in 1949 and, paraphrased in my words, defined it as "if a computer can fake a human good enough that another human cant notice a difference, then we have intelligence". On the less "talking" and more of the "intelligence" part of AI we have the [technological singularity](https://en.wikipedia.org/wiki/Technological_singularity) theorized by I.J Good (along side the previously mentioned Turing) in 1965.

And that was the era of computing I was born in to. When I started writing code at the ripe old age of single digits, I really did believe that eventually we would se machines being intelligent.

We were promised thinking machines - Actual artificial intelligence.

But that was _not_ what we got.

## The road to now

Somewhere in the earlier half of the 2000's the industry started talking about ["big data"](https://en.wikipedia.org/wiki/Big_data). The internet was exploding, gathering all the worlds knowledge in one place. Our hard drives at home grew to gigabytes (I miss you, [IBM DTLA3030](https://archive.org/details/manualsbase-id-70551)). 

Big data made sense to me. The converged amount of knowledge in the world is to big for any brain, so gathering and filtering was a logic next step.

Naturally, gathering big data could be used for evil. Given enough data about any one person it will of course be easy to figure out what their next step is. Advertising in the age of big data will be targeted, personal, and nightmarish. But in the end it can also be used for good. Enough data on where and how diseases spread makes it easier to create medicine. Enough data about what is where the environment is messed up makes it easier to see how to improve it. And of course - enough raw knowledge on all topics gathered in the history of human kind is a good skillset to start the road to true AI.

But that was _not_ what we got.

## What the hell is intelligence anyway?

Getting philosophical here.

Truth is - neither I or anyone else knows. Is intelligence just retelling what someone else told you? I think we all can agree that while this is a big part of learning just retelling things is not intelligence. Is it IQ? I sure hope not, as IQ testing (and by reference, [MENSA](https://www.mensa.org)) is an absolute joke measuring nothing but the skill of finding the last figure.

In fact - Intelligence should, if you ask me, be seen as something very much localized, and it is different everywhere and in every situation. Where I live, in Sweden, a "smart" or "intelligent" person may get a higher level education, read lots of books, or come up with the solution to some interesting computer problem. Sounds good, right?

Does that make the [Sentinelese](https://en.wikipedia.org/wiki/Sentinelese) unintelligent? AFAIK they have neither universities, Fredrik Nietzsche, or the computational power to calculate if the fifth figure is correct. They have, however, survived for thousands of years on an island in the pacific - something which most humans would not be able to do as they have no idea how the island works. Doesn't that make most humans unintelligent given the environment they live in? 

No - I believe intelligence is the art of drawing conclusions. For us Swedes this may mean learning about maths - not just repeating the times table but understanding why they are the way they are. It may be how to put on a band aid and when to call 112, or realizing that if my thermometer says '-10' I need a jacket. Some kind of intelligence is needed for all of these conclusions.

But that was _not_ what we got.

## So what did we get?

Some years ago - it may be 1, 2, 5, or 10, I am terrible with time - I first heard it uttered. "We have created an AI".
I was _terribly_ sceptical. After all - I work with computers. I spend my days with them. By now I do think I know enough to not be an amateur.

And I _know_ a computer cant be intelligent.. Simply because we don't know neither what it is or how it works. What makes our brains tick and click? What is the definition of intelligence? How do we draw a conclusion? And if we don't know _every single detail_ of this, then how can we replicate it to perfection in code? After all - it is _just_ code, written by humans. So what the hell is it you are saying here?

While I do not remember the first time I read about it, I do remember my first conversations about it.. And I remember my humble opinion was just "Hey, what they are describing here - isn't that just.. Big data?"

I remember a video I saw early on describing the training of early models. It was _incredibly_ impressive. A computer that could do approximations and relations between datapoints. But I do not believe this is the only thing that defines intelligence, right? There is so much more to it, right?

But no, that was what we got.

## But BjÃ¶rn, AI is so much more and smarter and better and...

Enter the LLM. The large language model. The concept of speech, and understanding how speech works. Just like with big data processing, this is an _amazing_ invention. Tokenize our language and we can suddenly see patterns in how it works - not just the old T9 "Guess the next word", but suddenly it handled word endings, relative relationships between sentences, and how we structured them. This really can revolutionize both text-to-speech and speech-to-text, and improve the life of people needing hearing or visual aids, for subtitling live tv, or.. 

Autocompletion in code!

But there is one thing that none of these LLM implementations have - Intelligence. The words in TTS/STT are uttered by a human. The code created is created by humans and replicated by an LLM. The LLM only needs to "know" that the token "Geno" is more likely to come after the token "Israel" if the next tokens are "cide" and "Gaza". "It" - The LLM - does _not_ need to _understand_ these things.

And it was in fact the combination of these two things we got.

## "AI"

They call it "intelligence" and claim that intelligence is simply the combination of "repeat all the data you have ever been told", and "use a glorified autocomplete to reply to you". They call it "hallucinations" when the reply simply is wrong. The users claim that "ChatGPT told me that ....".

And I feel like I am the only one asking - Was this really what I was promised when I was the ripe old age of single digits? Will this machine be able to invent it's own predecessor? To put it blunt - Since it is only a collection of big data combined with a fluent language, will this machine ever be able to invent.. anything?

And the answer I am afraid - is "no". Because "AI" doesn't contain any intelligence at all. It contains a lot of data, and given the right input it will be able to sort and filter that data, and since it has a natural language machine attached it may even be able to "explain" that data, but it can not draw conclusions. If the data isn't there, it can not give an educated guess. Instead it "hallucinates".

## And hallucinations we got

Why does the "AI" opposition call it the lying machine? Because a GPT/LLM can never truly admit it doesn't know. It just knows that "given this token, the following one is most likely.". This is why it invents sources that doesn't exist. This is why it invents commands that doesn't' exist. Because if the fist part of the sentence is "The most cited source for 1+1=3 is" then the most logic next token is a name. All of this is part of the concept of language models.

But it sounds damn confident when saying it, and humans, being, well, _actually_ intelligent, is genetically set to believe someone who sounds confident in a topic we are less skilled in.. Because if we _weren't_, we would not be able to draw the conclusion that "This person said big animal is deadly, So I best not pet big animal". In the end, hallucinations is nothing more than the model missing datapoints for your input, and the language model unable to tokenize the lack of those datapoints.

(Bonus from.. well.. today: Excel got a new Copilot plugin. It comes with a warning to not use it for calculations because it may hallucinate the answer. In a spreadsheet app. made for.. calculations.. To quote person from the internet: "How hard must it be to get a computer to _not_ be able to do maths!")

## But where is my AI?

Fast forward some more year(s) and we are in the now:ish era, and the tech bros started talking about ["AGI - Artificial General Intelligence"](https://en.wikipedia.org/wiki/Artificial_general_intelligence), and even ["ASI - Artificial Super intelligence](https://en.wikipedia.org/wiki/Superintelligence#Feasibility_of_artificial_superintelligence).

Apparently the answer to the question I was asking all that time - "Is this really intelligence?" - was "Yeah we just changed the definition of it all".

"My" AI was now upgrade to a new [TLA](https://en.wikipedia.org/wiki/Three-letter_acronym), and no, it wasn't invented. In fact, even though tech bros keep claiming we are _aaaaalmost there_, we still don't know what . _actual_ intelligence is. Tech bros keep repeating the it is just burping up datapoints, we just need more data!

I still don't believe them. AI (And I mean _real_ AI, not the re-definition of it) is not something that _can_ be solved by tech alone. It needs medical skills, philosophy, inference ability, and.. well, I don't know. Intelligence is to big. To philosophical.

Yet here I am ranting about it.

## So why does this make it hard?

Back to the beginning again. I find hard to write. Find it hard to code. Find it hard to invent. And why? Because AI.
Because whatever and wherever we turn we are to understand that AI is the only thing important. 

I am to understand that finding joy in actually writing code, writing a text, or reading a hundred blog posts to reach a conclusion that makes me understand something, all of this is wrong unless I have AI doing it for me. The entire industry says so. In fact - It is so sure of it that I am to be made redundant, fired, and I deserve it if I want to code because I enjoy it.

I am a bad person because I enjoy drawing dumb things in paint instead of using the new built in AI to draw it for me.

And telling me this works. It makes me loose my inspiration to code when I turn of the ChatGPT window in VSCode for the umpteenth time. And it makes it harder to write when friends, colleagues, and people I used to look up to keep telling me that "ChatGPT could write this blog post in a couple of minutes instead". 

And that's just half of it.

I spend a lot of time creating. When I write, even if it is a mail, I think a lot about which words to choose to get my point through. When I create a presentation I consider my words - both in slides, code, and spoken - carefully to make sure I say what I mean.

And it makes it hard to continue to do so when someone asks a LLM, a lying machine, to summarize it this mail or the video recording.

## And this is forced in to _everything_

At Microsofts biggest event of the year, Ignite 2024, there was about one session that wasn't either "We put AI in..." or "This is how you use AI to...". There was literally nothing new. And they are absolutely not alone. If we are to trust the IT business there is absolutely no need to create anything that isn't AI right now.

Don't get me wrong, I understand the concept of hype, and I understand the necessity of it as well in a capitalist society based on investors, but through every single hype I've seen, from VR to Blockchain, there has always been space for inventing. The AI hype doesn't have that space.

Instead it has Copilot in notes and paint...

Instead I have to close the VSCode ChatGPT window again because they added a new model or something.

## I am _not_ anti AI

I really am not.

For code I clearly see a place for it as the next version of snippets. It can help me finish a foreach loop with the correct variables in half the time it takes me to write it. But I do not find joy if my only action is "Hey lying machine, write this entire program for me".

For writing I clearly see use cases in the previously mentions live texting, improved spellchecks and corrections of language use, or for that matter - use it as an idea machine.

And I bet whatever business or hobby you have there are absolutely places where the concept of gather all the data in the world can help out. I play guitar, and god knows the Kempers and Neural DSPs could probably do some really strange and cool stuff given the entire history of sounds as input.

But I think it should be up to you and me if we want it. I still write my foreach loops by hand, because I like it. And I still use analogue guitar pedals, because I like them. But I do use subtitles because of bad hearing, and improving live ones would be a real game changer.

## Some last disclaimers

I wanted to do this some time ago, but even writing this was hard, because I _know_ the AI tech bros will have opinions.

So I'm going to address some of them here:

> If you don't get with the flow you are left behind! Understanding AI is a must!

Sure. I think I understand them as much as any other layman. I just find them... Uninspiring. If that costs me a career, then so be it. I'd rather enjoy life as a poor man than the other way around.

> All of the hallucinations is just because you write poor prompts! You need prompt engineering and inventing prompts is just as fun as inventing.... (go on forever about prompts here)

So.. I need to learn how to write words in a specific way in a specific order to get the computer to do what I want? Sounds... oddly familiar.. ðŸ¤” (The added bonus is sometimes it doesn't actually do what I tell it to..)

> Just don't use it then!
Believe me, I try, but as stated above - it is literally forced upon us. From Copilot buttons, to (unconfigurable) Gemini touch patterns, to companies believing the hype and going "If you don't use AI you are not productive!". I've even heard about a company that actually had "used amount of tokens" as a measurable in salary discussions.

> It increases productivity, and your company is about productivity, not you having fun!
Current research is _all but sure_ about this. There are lots of papers, articles, and anecdotes saying it decreases productivity as you have to bugfix bad code instead of writing good code. Adversely there is _a lot_ of research saying if you have fun at work your productivity will increase.

## So there it is. My rant

I know I have forgotten things I meant to write, and I have written things I have already forgotten. I haven't even touched on the impact of the environment, the blatant stealing everything from creators without any credit, or the fact that my work gets harder when I can't get Azure resources because they are all used for AI instead of actual useful things. 

In the end it turns out writing is, for me, easier if I just.. open the tap and let it flow. There are close to zero corrections in this post (except spelling and making sure I pick the correct words to get my point through). Close to zero going back and changing or adding something (except the parenthesis above..). Absolutely zero "Hey ChatGPT, can you change this sentence to sound more hateful". And I enjoyed writing it.

And I hope you enjoy reading it. Maybe, just maybe, you read something interesting. Something that made you think. Get an idea. "Maybe it all connects like this?".. Maybe, just maybe, you can draw some conclusion from it.

Until the AI bubble bursts, maybe this is what we will get.

//Bjompen

## Post scriptum

I just want to make a couple of last things clear:
- If you or your company us AI to generate add images, I will think less of you. You are cheap, uninteresting, and can't be bothered to spend a few minutes on creating something unique. A stick figure, a paint drawing, or a photo of whatever is a million times more inspiring than anything AI generated. Oh, and yes, it does show.
- If you receive any text, anything that I have written, or anything shared in a chat, and ask chatGPT to summarize it for you, I will think less of you. If you can not give me ten minutes to be correctly understood, then why should I care enough to give you ten minutes of my life?
- If you ever claim to "create" art, music, images, or even photo "improvements" using AI engines I will think less of you. What you are doing is the opposite of creating art. It is destroying the very soul of art. the humanity of it.

